{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations (à exécuter uniquement dans Google Colab)\n",
    "#!pip install torchmetrics\n",
    "#!pip install pytorch_lightning\n",
    "#!pip install transformers\n",
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\ft\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchmetrics\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from pprint import pprint\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from datasets import load_dataset\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cache dir for HuggingFace transformers library\n",
    "def get_cache_dir():\n",
    "    import sys\n",
    "    if \"linux\" in sys.platform:\n",
    "        # return \"/gfs/team/nlp/users/ait/.cache/\"\n",
    "        return \"/gfs-ssd/user/ait/.cache/\"\n",
    "    else:\n",
    "        return \"c:/Users/ait/.cache/\"\n",
    "\n",
    "HF_MODEL_CACHE_DIR = f\"{get_cache_dir()}/huggingface/transformers\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = HF_MODEL_CACHE_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, hfdataset_split, n: int, lmtokenizer, lb: LabelBinarizer):\n",
    "        # n is the number of (random) samples we want to use from this hf dataset\n",
    "        # shuffle and take the first n examples\n",
    "        data = hfdataset_split.shuffle(seed=123).select(range(n))\n",
    "        texts = [d['review_body'] for d in data]\n",
    "        encoded_texts = lmtokenizer(texts,\n",
    "                                    padding=False,\n",
    "                                    add_special_tokens=True,\n",
    "                                    return_tensors=None,\n",
    "                                    return_offsets_mapping=False,\n",
    "                                    )\n",
    "        self.input_ids = [torch.tensor(id_list) for id_list in encoded_texts['input_ids']]\n",
    "        self.attention_mask = [torch.tensor(mask_list) for mask_list in encoded_texts['attention_mask']]\n",
    "        # labels\n",
    "        labels = ['positive' if d['stars'] > 3 else 'negative' if d['stars'] < 3 else 'neutral' for d in data]\n",
    "        self.label_vects = torch.from_numpy(lb.transform(labels)).long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.input_ids[index], self.attention_mask[index], self.label_vects[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def collate_fn(self, batch_list):\n",
    "        # batch_list is a list of tuples, each returned by the __get_item__() function above\n",
    "        # create 3 separate lists for each element type in the tuples\n",
    "        input_ids, attention_masks, label_ids = tuple(zip(*batch_list))\n",
    "        # the batch will be a dictionary of tensors: a tensor for the input_ids, another for the attention_masks and another for the label_ids if any\n",
    "        batch = dict({})\n",
    "        batch['input_ids'] = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=1)\n",
    "        batch['attention_mask'] = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "        batch['label_vects'] = torch.nn.utils.rnn.pad_sequence(label_ids, batch_first=True, padding_value=0)\n",
    "        # return the batch as a dictionary of tensors\n",
    "        return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerClassifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hf_plm_name: str, output_size: int, dropout: float = 0.3):\n",
    "        # hf_plm_name = HuggingFace Pretrained Language Model name\n",
    "        super().__init__()\n",
    "        # text encoder/vectorizer: a pretrained language model\n",
    "        self.config = AutoConfig.from_pretrained(hf_plm_name)\n",
    "        lm_hidden_size = self.config.hidden_size\n",
    "        self.lm = AutoModel.from_pretrained(hf_plm_name, output_attentions=False)\n",
    "        # Linear layer(s) for the classifier component\n",
    "        self.fcn = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(lm_hidden_size, output_size),\n",
    "        )\n",
    "        # Loss function\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        # Learning rate\n",
    "        self.lr = 1e-4\n",
    "        #plus le model est grand plus le lR est petit \n",
    "\n",
    "    def forward(self, batch):\n",
    "        out = self.lm(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])[0]\n",
    "        out = self.fcn(out.mean(dim=1))\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step is called in PyTorch Lightning train loop\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss_fn(y_hat, batch['label_vects'])\n",
    "        self.log(\"loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def validation_step(self, batch, batch_ix):\n",
    "        # validation_step is called in PyTorch Lightning train loop\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss_fn(y_hat, batch['label_vects'])\n",
    "        acc = torchmetrics.functional.accuracy(y_hat, batch['label_vects'])\n",
    "        self.log_dict({'val_loss': loss.item(), 'val_acc': acc.item()}, on_step=False, on_epoch=True, reduce_fx='mean', prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        y_hat = self.forward(batch)\n",
    "        y_hat = F.softmax(y_hat)\n",
    "        test_acc = torchmetrics.functional.accuracy(y_hat, batch['label_vects'])\n",
    "        # test_loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"test_acc\", test_acc)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx: int = 0):\n",
    "        y_hat = self.forward(batch)\n",
    "        y_hat = F.softmax(y_hat)\n",
    "        return torch.round(y_hat).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Downloading builder script: 100%|██████████| 7.16k/7.16k [00:00<00:00, 2.39MB/s]\n",
      "Downloading metadata: 100%|██████████| 37.4k/37.4k [00:00<00:00, 360kB/s]\n",
      "Downloading readme: 100%|██████████| 13.4k/13.4k [00:00<00:00, 1.34MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset amazon_reviews_multi/fr to C:/Users/mosta/.cache/huggingface/datasets/amazon_reviews_multi/fr/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 81.9M/81.9M [01:00<00:00, 1.36MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [01:02<00:00, 62.06s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "Downloading data: 100%|██████████| 2.02M/2.02M [00:02<00:00, 904kB/s] \n",
      "Downloading data files: 100%|██████████| 1/1 [00:03<00:00,  3.88s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 502.25it/s]\n",
      "Downloading data: 100%|██████████| 2.04M/2.04M [00:03<00:00, 671kB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:04<00:00,  4.98s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 501.65it/s]\n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset amazon_reviews_multi downloaded and prepared to C:/Users/mosta/.cache/huggingface/datasets/amazon_reviews_multi/fr/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 16.09it/s]\n",
      "Parameter 'indices'=range(0, 50) of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Loading cached shuffled indices for dataset at C:/Users/mosta/.cache/huggingface/datasets/amazon_reviews_multi/fr/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609\\cache-1b41743e50f8d888.arrow\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\Anaconda\\envs\\ft\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1764: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: d:\\UGA\\M2\\Fouille de texte 'Ait'\\code\\transformer\\lightning_logs\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | lm      | CamembertModel   | 110 M \n",
      "1 | fcn     | Sequential       | 2.3 K \n",
      "2 | loss_fn | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.497   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\ft\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\ft\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "d:\\Anaconda\\envs\\ft\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1892: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  12%|█▎        | 1/8 [01:12<08:26, 72.41s/it, loss=5, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\ft\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "hf_plm_name = \"camembert-base\"\n",
    "# Define the tokenizer (for the pretrained language model)\n",
    "lmtokenizer = AutoTokenizer.from_pretrained(hf_plm_name)\n",
    "# Label binarizer in order to vectorize and devectorize labels\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(['positive', 'negative', 'neutral'])\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"amazon_reviews_multi\", \"fr\")\n",
    "train_dataset = ReviewDataset(dataset['train'], 50, lmtokenizer, lb)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=15, collate_fn=train_dataset.collate_fn, shuffle=False)\n",
    "val_dataset = ReviewDataset(dataset['train'], 100, lmtokenizer, lb)\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=15, collate_fn=train_dataset.collate_fn, shuffle=False)\n",
    "# Create the model\n",
    "model = TransformerClassifier(hf_plm_name, output_size=len(lb.classes_))\n",
    "# Training the model\n",
    "device = 'cpu'\n",
    "early_stop_callback = EarlyStopping(monitor='val_loss', min_delta=0.00, patience=5, verbose=True, mode='min')\n",
    "trainer = pl.Trainer(max_epochs=2,  callbacks=[early_stop_callback], log_every_n_steps=10, accelerator=device)\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ft')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6880d222d29fe4272ef7def451f48647ad45dfcc527b7822b0a0eef1aeaca23c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
